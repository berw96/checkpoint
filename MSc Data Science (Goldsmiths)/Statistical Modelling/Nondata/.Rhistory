filter_nondata <- function(df){
# scan provided dataset 'df' for nondata such as 'NA' or '?'.
# Once found, set it to 0 or "".
nondata_count = 0
for(j in 1:length(df)){
for(i in 1:length(df[,j])){
if(is.na(df[i,j]) || df[i,j] == '?'){
cat("Non-data detected ", "(", df[i,j], "), ", "at ", "[", i, ",", j, "]", "\n", sep = "")
nondata_count = nondata_count + 1
if(typeof(df[,j]) == "integer"){
print("Integer field")
df[i,j] = 0
} else if(typeof(df[,j]) == "double"){
print("Double or Numeric field")
df[i,j] = 0.0
} else if(typeof(df[,j]) == "character"){
print("Character field")
df[i,j] = ""
}
}
}
}
# Use recursion to rescan the dataframe for more nondata.
if(nondata_count != 0){
df = filter_nondata(df)
} else {
print("All nondata successfully removed.")
}
# If all nondata has been successfully removed, return the dataframe.
return(df)
}
library(Nondata)
library(Nondata)
library(Nondata)
data = read.csv("datasets/Auto.csv")
setwd("C:/Users/berw9/Desktop/git repos/checkpoint/MSc Data Science (Goldsmiths)/Statistical Modelling")
data = read.csv("datasets/Auto.csv")
scan_for_copies <- function(cops){
# scan the dataset for copies based on cylinders, model and year.
# Categorical copies skew statistical significance.
# Search through all observations in dataset.
for(i in 1:length(data$name)){
# Starting from the current observation 'i'
# scan through all remaining observations 'j'.
for(j in i:length(data$name)){
if(
i != j &&
data$cylinders[i] == data$cylinders[j] &&
data$name[i] == data$name[j] &&
data$year[i] == data$year[j]
){
print("Copy found")
# If a copy of 'i' is found, push both to the dataframe
cops = rbind(cops, data[i,], data[j,])
}
}
}
# Return a dataframe of duplicates as a result.
return(cops)
}
##############################################################################
# PREPROCESSING
# Create a dataframe to store duplicate car models.
copies = data.frame()
copies = scan_for_copies(copies)
# Remove duplicates from original based on index.
data = data[!(row.names(data) %in% row.names(copies)),]
# Interpret horsepower as an integer instead of a char.
copies[,4] = as.integer(copies[,4])
# Calculate the arithmetic mean of the duplicates'.
# First initialize an empty dataframe containing all fields.
mean_of_copies = data.frame(
mpg = c(0),
cylinders = c(0),
displacement = c(0),
horsepower = c(0),
weight = c(0),
acceleration = c(0),
year = c(copies$year[1]),
origin = c(copies$origin[1]),
name = (copies$name[1])
)
for(i in 1:6){
mean_of_copies[i] = mean(copies[,i])
}
# Append mean of duplicates to original dataframe.
data = rbind(data, mean_of_copies)
# Extract only the numeric data from our dataset.
# We exclude the 'name' column.
numeric_data = data[,-9]
# Convert numeric data represented in non-numeric form.
# Engine 'horsepower' is represented as 'chr' strings.
numeric_data[,4] = as.integer(numeric_data[,4])
# Calculate mean of horsepower excluding NA values.
mean_hp = mean(numeric_data[,4], na.rm = TRUE)
numeric_data = filter_nondata(numeric_data)
library(Nondata)
# Import Carseats.csv dataset into a representative variable.
# (Set as working directory first!)
data = read.csv("datasets/Carseats.csv")
##############################################################################
# PREPROCESSING
# Create dummy variables for each category in ShelveLoc.
# First convert to digits.
data$ShelveLoc = factor(
data$ShelveLoc,
levels = c("Bad", "Medium", "Good"),
labels = c(1,2,3)
)
# Encoding is Bad = 100, Medium = 001, and Good = 010
data$Bad_ShelveLoc    = ifelse(data$ShelveLoc == 1 ,1, 0)
data$Medium_ShelveLoc = ifelse(data$ShelveLoc == 2, 1, 0)
data$Good_ShelveLoc   = ifelse(data$ShelveLoc == 3, 1, 0)
# Encode categorical data in numeric form for use with training and testing.
data$Urban = factor(
data$Urban,
levels = c("No", "Yes"),
labels = c(0,1)
)
data$US = factor(
data$US,
levels = c("No", "Yes"),
labels = c(0,1)
)
# Store preprocessed data in a separate variable, excluding the now defunct ShelveLoc field.
numeric_data = subset(data, select = -c(7))
numeric_data = filter_nondata(numeric_data)
library(Nondata)
#install.packages('caTools')
library(caTools)
#install.packages('caret')
library(caret)
library(class)
#install.packages('Nondata')
library(Nondata)
round_fitted_values <- function(x){
for(i in 1:length(x)){
if(x[i] < 0.5){
x[i] = 0
} else if (x[i] >= 0.5){
x[i] = 1
}
}
return(x)
}
# Apply min-max normalization to continuous data.
min_max_norm <- function(x){
x = (x - min(x))/(max(x) - min(x))
return(x)
}
# Import dataset into a representative variable.
# (Set as working directory first!)
data = read.csv("datasets/Weekly.csv")
summary(data)
##############################################################################
# PREPROCESSING
# Data encoding.
# Run this for all data variables!
data$Direction = factor(
data$Direction,
levels = c("Down", "Up"),
labels = c(0,1)
)
# Normalize data. Values scaled to between 0 and 1 with relationship being
# conserved, but at the expense of outlier visibility.
for(i in 2:8){
data[,i] = min_max_norm(data[,i])
}
data = filter_nondata(data)
