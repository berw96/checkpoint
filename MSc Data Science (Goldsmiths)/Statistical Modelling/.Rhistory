plot(Volume)
# categorical data is predicted using logistic regression
# thus we can predict 'Direction'
glm.fit = glm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
data = sm,
family = binomial)
# see the results of our logistic regression model
summary(glm.fit)
# access only the coefficients of the model
coef(glm.fit)
# print a summary for these coefficients
summary(glm.fit)$coef
summary(glm.fit)$coef[,4]
# predict the direction of the market
glm.probs = predict(object =  glm.fit,
type = "response")
glm.probs[1:10]
# detail the discrete values associated with each market direction
contrasts(Direction)
glm.pred = rep("Down", 1250)
glm.pred[glm.probs > 0.5] = "Up"
table(glm.pred, Direction)
mean(glm.pred == Direction)
train = (Year < 2005)
sm.2005 = sm[!train,]
dim(sm.2005)
Direction.2005 = Direction[!train]
glm.fit = glm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
data = sm,
family = binomial,
subset = train)
glm.probs = predict(object = glm.fit,
sm.2005,
type = "response")
View(glm.fit)
View(sm.2005)
glm.fit = glm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
data = sm,
family = binomial,
subset = train)
glm.probs = predict(object = glm.fit,
newdata = sm.2005,
type = "response")
glm.pred = rep("Down", 252)
glm.pred[glm.probs > 0.5] = "Up"
table(glm.pred, Direction.2005)
mean(glm.pred == Direction.2005)
glm.fit = glm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
data = sm,
family = binomial,
subset = train)
glm.probs = predict(object = glm.fit,
newdata = sm.2005,
type = "response")
glm.pred = rep("Down", 252)
glm.pred[glm.probs > 0.5] = "Up"
table(glm.pred, Direction.2005)
mean(glm.pred == Direction.2005)
mean(glm.pred != Direction.2005)
View(glm.fit)
library(ISLR)
names(Smarket)
# reduce character count of data set
sm = Smarket
# print the dimensions of our data set
dim(sm)
# print a summary of our data set
summary(sm)
# create a scatter plot matrix for our data set
pairs(sm)
# create a correlation matrix containing all entries/rows
# and discounting the 10th field/column 'Direction'
cor(sm[,-9])
# create a dot plot using the data set
attach(sm)
plot(Volume)
# categorical data is predicted using logistic regression
# thus we can predict 'Direction'
glm.fit = glm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
data = sm,
family = binomial)
# see the results of our logistic regression model
summary(glm.fit)
# access only the coefficients of the model
coef(glm.fit)
# print a summary for these coefficients
summary(glm.fit)$coef
summary(glm.fit)$coef[,4]
# predict the direction of the market
glm.probs = predict(object =  glm.fit,
type = "response")
glm.probs[1:10]
# detail the discrete values associated with each market direction
contrasts(Direction)
glm.pred = rep("Down", 1250)
glm.pred[glm.probs > 0.5] = "Up"
table(glm.pred, Direction)
mean(glm.pred == Direction)
train = (Year < 2005)
sm.2005 = sm[!train,]
dim(sm.2005)
Direction.2005 = Direction[!train]
glm.fit = glm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
data = sm,
family = binomial,
subset = train)
glm.probs = predict(object = glm.fit,
newdata = sm.2005,
type = "response")
glm.pred = rep("Down", 252)
glm.pred[glm.probs > 0.5] = "Up"
table(glm.pred, Direction.2005)
mean(glm.pred == Direction.2005)
mean(glm.pred != Direction.2005)
# now perform backward elimination to enhance
# accuracy of model by removing statistically
# insignificant coefficients
glm.fit = glm(formula = Direction ~ Lag1 + Lag2,
data = sm,
family = binomial,
subset = train)
glm.probs = predict(object = glm.fit,
newdata = sm.2005,
type = "response")
glm.pred = rep("Down", 252)
glm.pred[glm.probs > 0.5] = "Up"
table(glm.pred, Direction.2005)
mean(glm.pred == Direction.2005)
library(MASS)
library(ISLR)
names(Smarket)
sm = Smarket
lda.fit = lda(formula = Direction ~ Lag1 + Lag2,
data = sm,
subset = train)
lda.fit
library(MASS)
library(ISLR)
names(Smarket)
sm = Smarket
lda.fit = lda(formula = Direction ~ Lag1 + Lag2,
data = sm,
subset = train)
lda.fit
library(ISLR)
names(Smarket)
# reduce character count of data set
sm = Smarket
# print the dimensions of our data set
dim(sm)
# print a summary of our data set
summary(sm)
# create a scatter plot matrix for our data set
pairs(sm)
# create a correlation matrix containing all entries/rows
# and discounting the 10th field/column 'Direction'
cor(sm[,-9])
# create a dot plot using the data set
attach(sm)
plot(Volume)
# categorical data is predicted using logistic regression
# thus we can predict 'Direction'
glm.fit = glm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
data = sm,
family = binomial)
# see the results of our logistic regression model
summary(glm.fit)
# access only the coefficients of the model
coef(glm.fit)
# print a summary for these coefficients
summary(glm.fit)$coef
summary(glm.fit)$coef[,4]
# predict the direction of the market
glm.probs = predict(object =  glm.fit,
type = "response")
glm.probs[1:10]
# detail the discrete values associated with each market direction
contrasts(Direction)
glm.pred = rep("Down", 1250)
glm.pred[glm.probs > 0.5] = "Up"
table(glm.pred, Direction)
mean(glm.pred == Direction)
train = (Year < 2005)
sm.2005 = sm[!train,]
dim(sm.2005)
Direction.2005 = Direction[!train]
glm.fit = glm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
data = sm,
family = binomial,
subset = train)
glm.probs = predict(object = glm.fit,
newdata = sm.2005,
type = "response")
glm.pred = rep("Down", 252)
glm.pred[glm.probs > 0.5] = "Up"
table(glm.pred, Direction.2005)
mean(glm.pred == Direction.2005)
mean(glm.pred != Direction.2005)
# now perform backward elimination to enhance
# accuracy of model by removing statistically
# insignificant coefficients
glm.fit = glm(formula = Direction ~ Lag1 + Lag2,
data = sm,
family = binomial,
subset = train)
glm.probs = predict(object = glm.fit,
newdata = sm.2005,
type = "response")
glm.pred = rep("Down", 252)
glm.pred[glm.probs > 0.5] = "Up"
table(glm.pred, Direction.2005)
mean(glm.pred == Direction.2005)
# Linear Discriminant Analysis
library(MASS)
lda.fit = lda(formula = Direction ~ Lag1 + Lag2,
data = sm,
subset = train)
lda.fit
plot(lda.fit)
lda.pred = predict(object = lda.fit,
newdata = sm.2005)
names(lda.pred)
View(lda.fit)
mean(lda.clss == Direction.2005)
lda.class = lda.pred$class
table(lda.class, Direction.2005)
mean(lda.class == Direction.2005)
sum(lda.pred$posterior[,1] >= 0.5)
sum(lda.pred$posterior[,1] < 0.5)
lda.pred$posterior[1:20,1]
lda.class[1:20]
setwd("C:/Users/berw9/OneDrive/Desktop/git repos/checkpoint/MSc Data Science (Goldsmiths)/Statistical Modelling")
data = read.csv("datasets/Weekly.csv")
summary(data)
plot(data)
data$Direction = factor(
data$Direction,
levels = c("Down", "Up"),
labels = c(0,1)
)
min_max_norm <- function(x){
x = (x - min(x))/(max(x) - min(x))
return(x)
}
for(i in 2:6){
data[,i] = min_max_norm(data[,i])
}
min_max_norm <- function(x){
x = (x - min(x))/(max(x) - min(x))
return(x)
}
for(i in 2:8){
data[,i] = min_max_norm(data[,i])
}
library(caTools)
set.seed(123)
split = sample.split(data$Direction, SplitRatio = 0.75)
train = subset(data, split == TRUE)
test = subset(data, split == FALSE)
lr_model = glm(
formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
data = train,
family = binomial)
summary(lr_model)
actual_values = train$Direction
for(i in 1:length(lr_model$fitted.values)){
if(lr_model$fitted.values[i] < 0.5){
lr_model$fitted.values[i] = 0
} else if (lr_model$fitted.values[i] >= 0.5){
lr_model$fitted.values[i] = 1
}
}
# Interpret the fitted values of our model as a factor.
predicted_values = factor(
lr_model$fitted.values,
levels = c(0,1),
labels = c(0,1)
)
confusionMatrix(data = actual_values, reference = predicted_values)
library(caret)
confusionMatrix(data = actual_values, reference = predicted_values)
print(lr_model$fitted.values)
probabilities = predict(
lr_model,
type = 'response',
newdata = test
)
probabilities
predictions = ifelse(probabilities >= 0.5, 1, 0)
predictions
confusionMatrix(data = test, reference = predictions)
confusionMatrix(data = test$Direction, reference = predictions)
train_actual_values = train$Direction
test_actual_values = factor(
test$Direction,
levels = c(0,1),
labels = c(0,1)
)
remove(actual_values)
confusionMatrix(data = test_actual_values, reference = as.factor(predictions))
test_actual_values = test$Direction
confusionMatrix(data = test_actual_values, reference = as.factor(predictions))
test_actual_values = test$Direction
confusionMatrix(data = test_actual_values, reference = as.factor(predictions))
predicted_values = lr_model$fitted.values
confusionMatrix(data = train_actual_values, reference = predicted_values)
# Compute confusion matrix for model.
confusionMatrix(data = train_actual_values, reference = as.factor(predicted_values))
confusionMatrix(data = test_actual_values, reference = as.factor(predictions))
confusionMatrix(data = train_actual_values, reference = as.factor(predicted_values))
data2 = data
data2 = subset(data2, data2$Year > 1990)
data2 = subset(data2, data2$Year <= 2008)
lr_model2 = glm(
formula = Direction ~ Lag2,
data = train2,
family = binomial
)
split = sample.split(data2$Direction, SplitRatio = 0.75)
train2 = subset(data2, split == TRUE)
test2 = subset(data2, split == FALSE)
lr_model2 = glm(
formula = Direction ~ Lag2,
data = train2,
family = binomial
)
actual_values2 = train2$Direction
lr_model2$fitted.values
for(i in 1:length(lr_model2$fitted.values)){
if(lr_model2$fitted.values[i] < 0.5){
lr_model2$fitted.values[i] = 0
} else if (lr_model2$fitted.values[i] >= 0.5){
lr_model2$fitted.values[i] = 1
}
}
lr_model2$fitted.values
confusionMatrix(data = actual_values2, reference = as.factor(predicted_values2))
predicted_values2 = lr_model2$fitted.values
confusionMatrix(data = actual_values2, reference = as.factor(predicted_values2))
summary(lr_model2)
summary(lr_model)
data3 = data
data3 = subset(data3, data3$Year >= 2009)
data3 = subset(data3, data3$Year <= 2010)
split = sample.split(data3$Direction, SplitRatio = 0.75)
train3 = subset(data3, split == TRUE)
test3 = subset(data3, split == FALSE)
lr_model3 = glm(
formula = Direction ~ Lag2,
data = train3,
family = binomial
)
summary(lr_model3)
plot(data3$Year, data3$Direction)
plot(data3$Year, data3$Volume)
probabilities2 = predict(
lr_model2,
type = 'response',
newdata = test2
)
predictions2 = ifelse(probabilities2 >= 0.5, 1, 0)
train_actual_values2 = train2$Direction
remove(actual_values2)
predictions2 = ifelse(probabilities2 >= 0.5, 1, 0)
test_actual_values2 = test2$Direction
confusionMatrix(data = test_actual_values2, reference = as.factor(predictions2))
predicted_values
round_fitted_values <- function(x){
for(i in 1:length(x)){
if(x[i] < 0.5){
x[i] = 0
} else if (x[i] >= 0.5){
x[i] = 1
}
}
return(x)
}
lr_model2$fitted.values = round_fitted_values(lr_model2$fitted.values)
predicted_values2 = lr_model2$fitted.values
confusionMatrix(data = train_actual_values2, reference = as.factor(predicted_values2))
# Round fitted values for a given regressional model.
round_fitted_values <- function(x){
for(i in 1:length(x)){
if(x[i] < 0.5){
x[i] = 0
} else if (x[i] >= 0.5){
x[i] = 1
}
}
return(x)
}
# Apply min-max normalization to continuous data.
min_max_norm <- function(x){
x = (x - min(x))/(max(x) - min(x))
return(x)
}
lr_model3$fitted.values = round_fitted_values(lr_model3$fitted.values)
lr_model3$fitted.values
confusionMatrix(data = train_actual_values3, reference = as.factor(predicted_values3))
train_actual_values3 = train3$Direction
predicted_values3 = lr_model3$fitted.values
confusionMatrix(data = train_actual_values3, reference = as.factor(predicted_values3))
probabilities3 = predict(
lr_model3,
type = 'response',
newdata = test3
)
predictions3 = ifelse(probabilities3 >= 0.5, 1, 0)
test_actual_values3 = test3$Direction
confusionMatrix(data = test_actual_values3, reference = as.factor(predictions3))
summary(lr_model3)
setwd("C:/Users/berw9/OneDrive/Desktop/git repos/checkpoint/MSc Data Science (Goldsmiths)/R Testing Grounds")
data = read.csv("datasets/Weekly.csv")
setwd("C:/Users/berw9/OneDrive/Desktop/git repos/checkpoint/MSc Data Science (Goldsmiths)/R Testing Grounds")
data = read.csv("datasets/Weekly.csv")
data = read.csv("../Statistical Modelling/datasets/Weekly.csv")
dataset = read.csv("../Statistical Modelling/datasets/Weekly.csv")
dataset = read.csv("../Statistical Modelling/datasets/Weekly.csv")
library(class)
install.packages('ElemStatLearn')
library(class)
library(caTools)
dataset$Direction = factor(
dataset$Direction,
levels = c("Down", "Up"),
labels = c(0,1)
)
for(i in 2:8){
dataset[,i] = min_max_norm(data[,i])
}
View(dataset)
split = sample.split(dataset$Direction, SplitRatio = 0.75)
train = subset(dataset, split == TRUE)
test = subset(dataset, split == FALSE)
predictions = knn(
train = train,
test = test,
train$Direction,
k = 5
)
summary(predictions)
install.packages('ElemStatLearn')
cm = table(test$Direction, predictions)
cm
install.packages("C:/Users/berw9/Downloads/ElemStatLearn_2015.6.26.tar.gz", repos = NULL, type = "source")
summary(predictions)
predictions
View(test)
View(train)
setwd("C:/Users/berw9/OneDrive/Desktop/git repos/checkpoint/MSc Data Science (Goldsmiths)/Statistical Modelling")
library(caTools)
set.seed(123)
data_KNN = read.csv("datasets/Weekly.csv")
data_KNN = subset(data_KNN, data_KNN$Year > 1990)
data_KNN = subset(data_KNN, data_KNN$Year <= 2008)
split = sample.split(data_KNN$Direction, SplitRatio = 0.75)
train_KNN = subset(data_KNN, split == TRUE)
test_KNN = subset(data_KNN, split == FALSE)
data_KNN$Direction = factor(
data_KNN$Direction,
levels = c("Down", "Up"),
labels = c(0,1)
)
split = sample.split(data_KNN$Direction, SplitRatio = 0.75)
train_KNN = subset(data_KNN, split == TRUE)
test_KNN = subset(data_KNN, split == FALSE)
View(data_KNN)
View(test_KNN)
View(train_KNN)
predictions_KNN = knn(
train = train_KNN,
test = test_KNN,
train_KNN$Direction,
k = 5
)
cm = table(test_KNN$Direction, predictions_KNN)
predictions_KNN = knn(
train = train_KNN,
test = test_KNN,
train_KNN$Direction,
k = 1
)
cm = table(test_KNN$Direction, predictions_KNN)
cm
predictions_KNN = knn(
train = train_KNN,
test = test_KNN,
train_KNN$Direction,
k = 3
)
cm
cm = table(test_KNN$Direction, predictions_KNN)
cm
predictions_KNN = knn(
train = train_KNN,
test = test_KNN,
train_KNN$Direction,
k = 31
)
cm = table(test_KNN$Direction, predictions_KNN)
cm
predictions_KNN = knn(
train = train_KNN,
test = test_KNN,
train_KNN$Direction,
k = 7
)
cm = table(test_KNN$Direction, predictions_KNN)
cm
predictions_KNN
confusionMatrix(data = test_KNN$Direction, reference =  predictions_KNN)
set.seed(123)
